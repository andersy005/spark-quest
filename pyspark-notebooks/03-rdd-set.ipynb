{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png) ![](https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Operations on RDDs\n",
    "===============================\n",
    "\n",
    "\n",
    "\n",
    "Spark supports many of the operations we have in mathematical sets, such as union and intersection, even when the RDDs themselves are not properly sets. It is important to note that these operations require that the RDDs being operated on are of the same type.\n",
    "\n",
    "\n",
    "Set operations are quite straightforward to understand as it work as expected. The only consideration comes from the fact that RDDs are not real sets, and therefore operations such as the union of RDDs doesn't remove duplicates. \n",
    "- subtract, \n",
    "- distinct, and \n",
    "- cartesian.\n",
    "\n",
    "## 1. Getting the data and creating the RDD\n",
    "\n",
    "In this case we will use the complete dataset provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \n",
    "                        \"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting attack interactions using ```subtract```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_raw_data = raw_data.subtract(normal_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some counts to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All count in 16.635 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# count all\n",
    "t0 = time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time() - t0\n",
    "\n",
    "print \"All count in {} secs\".format(round(tt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count in 16.226 secs\n"
     ]
    }
   ],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"Normal count in {} secs\".format(round(tt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack count in 114.181 secs\n"
     ]
    }
   ],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"Attack count in {} secs\".format(round(tt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 972781 normal interactions and 3925650 attacks, from a total of 4898431 interactions\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} normal interactions and {} attacks, \\\n",
    "from a total of {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Protocol and service combinations using ```cartesian```\n",
    "\n",
    "We can compute the Cartesian product between two RDDs by using the ```cartesian``` transformation. It returns all possible pairs of elements between two RDDs.\n",
    "\n",
    "In our case we will use it to generate all the possible combinations between service and protocol in our network interactions.\n",
    "\n",
    "First of all we need to isolate each collection of values in two separate RDDs. For that we will use distinct on the CSV-parsed dataset. From the dataset description we know that protocol is the second column and service is the third (tag is the last one and not the first as appears in the page).\n",
    "\n",
    "\n",
    "**dataset description:**\n",
    "```json\n",
    "back,buffer_overflow,ftp_write,guess_passwd,imap,ipsweep,land,loadmodule,multihop,neptune,nmap,normal,perl,phf,pod,portsweep,rootkit,satan,smurf,spy,teardrop,warezclient,warezmaster.\n",
    "duration: continuous.\n",
    "protocol_type: symbolic.\n",
    "service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'udp', u'icmp', u'tcp']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the protocols\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
    "protocols.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'urp_i',\n",
       " u'http_443',\n",
       " u'Z39_50',\n",
       " u'smtp',\n",
       " u'domain',\n",
       " u'private',\n",
       " u'echo',\n",
       " u'time',\n",
       " u'shell',\n",
       " u'red_i',\n",
       " u'eco_i',\n",
       " u'sunrpc',\n",
       " u'ftp_data',\n",
       " u'urh_i',\n",
       " u'pm_dump',\n",
       " u'pop_3',\n",
       " u'pop_2',\n",
       " u'systat',\n",
       " u'ftp',\n",
       " u'uucp',\n",
       " u'whois',\n",
       " u'harvest',\n",
       " u'netbios_dgm',\n",
       " u'efs',\n",
       " u'remote_job',\n",
       " u'daytime',\n",
       " u'ntp_u',\n",
       " u'finger',\n",
       " u'ldap',\n",
       " u'netbios_ns',\n",
       " u'kshell',\n",
       " u'iso_tsap',\n",
       " u'ecr_i',\n",
       " u'nntp',\n",
       " u'http_2784',\n",
       " u'printer',\n",
       " u'domain_u',\n",
       " u'uucp_path',\n",
       " u'courier',\n",
       " u'exec',\n",
       " u'aol',\n",
       " u'netstat',\n",
       " u'telnet',\n",
       " u'gopher',\n",
       " u'rje',\n",
       " u'sql_net',\n",
       " u'link',\n",
       " u'ssh',\n",
       " u'netbios_ssn',\n",
       " u'csnet_ns',\n",
       " u'X11',\n",
       " u'IRC',\n",
       " u'tftp_u',\n",
       " u'login',\n",
       " u'supdup',\n",
       " u'name',\n",
       " u'nnsp',\n",
       " u'mtp',\n",
       " u'http',\n",
       " u'bgp',\n",
       " u'ctf',\n",
       " u'hostnames',\n",
       " u'klogin',\n",
       " u'vmnet',\n",
       " u'tim_i',\n",
       " u'discard',\n",
       " u'imap4',\n",
       " u'auth',\n",
       " u'other',\n",
       " u'http_8001']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Services\n",
    "services = csv_data.map(lambda x: x[2]).distinct()\n",
    "services.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the cartesian product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 210 combinations of protocol X service\n"
     ]
    }
   ],
   "source": [
    "product = protocols.cartesian(services).collect()\n",
    "print \"There are {} combinations of protocol X service\".format(len(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, for such small RDDs doesn't really make sense to use Spark cartesian product. We could have perfectly collected the values after using distinct and do the cartesian product locally. Moreover, distinct and cartesian are expensive operations so they must be used with care when the operating datasets are large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
