{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals 1 - Introduction to Spark</h1>\n",
    "<h2 align = \"center\"> Getting Started</h2>\n",
    "<h4 align = \"center\"> April 11, 2017 </h4>\n",
    "<br align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png) ![](https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg) ![]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-04-12 05:06:39--  https://ibm.box.com/shared/static/1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip\n",
      "Resolving ibm.box.com (ibm.box.com)... 107.152.27.197\n",
      "Connecting to ibm.box.com (ibm.box.com)|107.152.27.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://ibm.ent.box.com/shared/static/1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip [following]\n",
      "--2017-04-12 05:06:39--  https://ibm.ent.box.com/shared/static/1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip\n",
      "Resolving ibm.ent.box.com (ibm.ent.box.com)... 107.152.26.211\n",
      "Connecting to ibm.ent.box.com (ibm.ent.box.com)|107.152.26.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://public.boxcloud.com/d/1/lF6oUONnSZlKtI2og4oQXr_Qh_zu0XHWUP0I4BUu3DPVMZ3MGZPF3K9Rg_M6apQl1eYtSZyopmdaA0QubL5CaQoW5-WI47RdOUGNtys-lgEToifrZLx_ph6AiGoyCEp_KBDhUg9yxIQ71xRLtBPANGg7TDPru3VMdcgafYkUdDbDs7Gons5TcwkUR5z9IuV1sJ--6zMuuapHDbeU0I658u4zNr5uPl15xNiDx3j6i6pBAEiKfLhFU0mzuxDLgTAKHh_oqhfMF9IaAzCHr9XVgWvLrh4V0F-IrYEcbLIpjtKCfp-19vASgiBUms1UwMC6hyuy8NB352Oc1UG_r8SH1niq11OqUq4z5Suq-CkNZ4mANaOv-H8vPBXkvDVTcPCS4eLQERaHnBozKHAyZd7x3T3v3-aOTAVhTT11v_mic77TjtL9CnsZC9oPY--2VzNNpWuqPzcB3uVp4MTjgwcLNLoBazUutdDOaQ0flzHzLJf_nnW-KAbqybur1IhudAxurXP9siKhK__1cT3gyXQCaSH1YB71pb4ba3tC6M-5wwPrUKZYnWGEazabaZ3cxP7RdWTYcRWXfLQ3UeyE5iRbkuSOo0z0jaxfGTc5zlaXp39P6_s76wc7U252dRwzZSC5bzOVvvZvqb1w1cvRY3KviLqbcryA1HqbKYuwsM0yEXYpdu431V0fsxaQBz3swsY4h536l9-uzWFoOctBfvyWNwGCjrnsS4kQ0r2Z03Srkf2vvgnk9c4OFLgcfSTy0jmHY-s6OqwhqW9w5b2ovcbpZAREWtvLWiRTTfr_liLATRX0IqYiJGxikzTwv_Nz6Xl4_6XpOFKfuREy1RiHYYqDxieb3bLgcLYSw48vIMS5_FN-u11ZMDoNKKNJb-UlCgdbdUe9iDQ3CCk-Byu0kgFUdFquCf7iyqYczbCwaeyLNTnEQ3WSg9TTVtFNM4_AEQ0lmASzsR0Fsun0xXfNjdkG3VjdPX6LhrBShMRdXbM4BVlP02VEukCy0i9HpsIO_tylnz99C7CXHkxFcBz6XotXG15M9eCp1OPPUFmLPmnQRGKdL7kmmZk-ZtZVOobzBbN93uqVrMPSr0bx8gkFwXIR5ppyqvlRBowt7xY_lJ76kuFOjlR2_4Wz-LgtCfjGHjSJ-Fia1iEyJEyEERNeAdevrTCtloqJYVYHc0JjaLWZCkNpOBPZPHkm2k1xX6h80e07cFtXFUp4MA../download [following]\n",
      "--2017-04-12 05:06:40--  https://public.boxcloud.com/d/1/lF6oUONnSZlKtI2og4oQXr_Qh_zu0XHWUP0I4BUu3DPVMZ3MGZPF3K9Rg_M6apQl1eYtSZyopmdaA0QubL5CaQoW5-WI47RdOUGNtys-lgEToifrZLx_ph6AiGoyCEp_KBDhUg9yxIQ71xRLtBPANGg7TDPru3VMdcgafYkUdDbDs7Gons5TcwkUR5z9IuV1sJ--6zMuuapHDbeU0I658u4zNr5uPl15xNiDx3j6i6pBAEiKfLhFU0mzuxDLgTAKHh_oqhfMF9IaAzCHr9XVgWvLrh4V0F-IrYEcbLIpjtKCfp-19vASgiBUms1UwMC6hyuy8NB352Oc1UG_r8SH1niq11OqUq4z5Suq-CkNZ4mANaOv-H8vPBXkvDVTcPCS4eLQERaHnBozKHAyZd7x3T3v3-aOTAVhTT11v_mic77TjtL9CnsZC9oPY--2VzNNpWuqPzcB3uVp4MTjgwcLNLoBazUutdDOaQ0flzHzLJf_nnW-KAbqybur1IhudAxurXP9siKhK__1cT3gyXQCaSH1YB71pb4ba3tC6M-5wwPrUKZYnWGEazabaZ3cxP7RdWTYcRWXfLQ3UeyE5iRbkuSOo0z0jaxfGTc5zlaXp39P6_s76wc7U252dRwzZSC5bzOVvvZvqb1w1cvRY3KviLqbcryA1HqbKYuwsM0yEXYpdu431V0fsxaQBz3swsY4h536l9-uzWFoOctBfvyWNwGCjrnsS4kQ0r2Z03Srkf2vvgnk9c4OFLgcfSTy0jmHY-s6OqwhqW9w5b2ovcbpZAREWtvLWiRTTfr_liLATRX0IqYiJGxikzTwv_Nz6Xl4_6XpOFKfuREy1RiHYYqDxieb3bLgcLYSw48vIMS5_FN-u11ZMDoNKKNJb-UlCgdbdUe9iDQ3CCk-Byu0kgFUdFquCf7iyqYczbCwaeyLNTnEQ3WSg9TTVtFNM4_AEQ0lmASzsR0Fsun0xXfNjdkG3VjdPX6LhrBShMRdXbM4BVlP02VEukCy0i9HpsIO_tylnz99C7CXHkxFcBz6XotXG15M9eCp1OPPUFmLPmnQRGKdL7kmmZk-ZtZVOobzBbN93uqVrMPSr0bx8gkFwXIR5ppyqvlRBowt7xY_lJ76kuFOjlR2_4Wz-LgtCfjGHjSJ-Fia1iEyJEyEERNeAdevrTCtloqJYVYHc0JjaLWZCkNpOBPZPHkm2k1xX6h80e07cFtXFUp4MA../download\n",
      "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.27.200\n",
      "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.27.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 342130521 (326M) [application/zip]\n",
      "Saving to: ‘1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip.2’\n",
      "\n",
      "100%[======================================>] 342,130,521 36.5MB/s   in 9.5s   \n",
      "\n",
      "2017-04-12 05:06:50 (34.2 MB/s) - ‘1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip.2’ saved [342130521/342130521]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ibm.box.com/shared/static/1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip\n",
      "  inflating: /resources/LabData/.DS_Store  \n",
      "  inflating: /resources/__MACOSX/LabData/._.DS_Store  \n",
      "  inflating: /resources/LabData/followers.txt  \n",
      "  inflating: /resources/__MACOSX/LabData/._followers.txt  \n",
      "  inflating: /resources/LabData/notebook.log  \n",
      "  inflating: /resources/__MACOSX/LabData/._notebook.log  \n",
      "  inflating: /resources/LabData/nyctaxi.csv  \n",
      "  inflating: /resources/__MACOSX/LabData/._nyctaxi.csv  \n",
      "  inflating: /resources/LabData/nyctaxi100.csv  \n",
      "  inflating: /resources/__MACOSX/LabData/._nyctaxi100.csv  \n",
      "  inflating: /resources/LabData/nyctaxisub.csv  \n",
      "  inflating: /resources/__MACOSX/LabData/._nyctaxisub.csv  \n",
      "  inflating: /resources/LabData/nycweather.csv  \n",
      "  inflating: /resources/__MACOSX/LabData/._nycweather.csv  \n",
      "  inflating: /resources/LabData/pom.xml  \n",
      "  inflating: /resources/__MACOSX/LabData/._pom.xml  \n",
      "  inflating: /resources/LabData/README.md  \n",
      "  inflating: /resources/__MACOSX/LabData/._README.md  \n",
      "  inflating: /resources/LabData/taxistreams.py  \n",
      "  inflating: /resources/__MACOSX/LabData/._taxistreams.py  \n",
      "  inflating: /resources/LabData/users.txt  \n",
      "  inflating: /resources/__MACOSX/LabData/._users.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o -d /resources 1c65hfqjxyxpdkts42oab8i8mzxbpvc8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers.txt\r\n",
      "notebook.log\r\n",
      "nyctaxi100.csv\r\n",
      "nyctaxi.csv\r\n",
      "nyctaxisub.csv\r\n",
      "nycweather.csv\r\n",
      "pom.xml\r\n",
      "README.md\r\n",
      "taxistreams.py\r\n",
      "users.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /resources/LabData/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Starting with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readme = sc.textFile(\"/resources/LabData/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some RDD actions on this text file. Count the number of items in the RDD using this command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try a transformation. Use the filter transformation to return a new RDD with a subset of the items in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"spark\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"Spark\" in line)\n",
    "readme.filter(lambda line: \"Spark\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD can be used for more complex computations. To find the line from the readme file with most words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first maps a line to an integer value, the number of words in that line\n",
    "- The second part reduce is called to find the line with the most words in it.\n",
    "\n",
    "The arguments to map and reduce are Python anonymous functions (lambdas), but you can use any top level Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a MapReduce data flow pattern. We can use this to do a word count on the readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts = readme.flatMap(lambda line: line.split())\\\n",
    "                   .map(lambda word: (word, 1))\\\n",
    "                   .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 3),\n",
       " (u'Scala,', 1),\n",
       " (u'environment', 1),\n",
       " (u'only', 1),\n",
       " (u'rich', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combined the flatMap, map, and the reduceByKey functions to do a word count of each word in the readme file.\n",
    "\n",
    "To collect the word counts, use the collect action.\n",
    "\n",
    "####It should be noted that the collect function brings all of the data into the driver node. For a small dataset, this is acceptable but, for a large dataset this can cause an Out Of Memory error. It is recommended to use collect() for testing only. The safer approach is to use the take() function e.g. print take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = readme.flatMap(lambda line: line.split())\\\n",
    "           .map(lambda word: (word, 1))\\\n",
    "           .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1),\n",
       " (u'using:', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# swap k, v to v, k to sort by word frequency\n",
    "swap = lambda x: (x[1], x[0])\n",
    "wc_swap = wc.map(swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u'when'),\n",
       " (1, u'R,'),\n",
       " (3, u'including'),\n",
       " (1, u'computation'),\n",
       " (1, u'using:')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_swap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the keys by ascending=false (descending)\n",
    "freq = wc_swap.sortByKey(False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, u'the'), (14, u'Spark'), (14, u'to'), (12, u'for'), (10, u'and')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'the', 21)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.reduce(lambda a, b: a if (a[1] > b[1]) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark caching\n",
    "\n",
    "Spark caching can be used to pull data sets into a cluster-wide in- memory cache. This is very useful for accessing repeated data, such as querying a small “hot” dataset or when running an iterative algorithm. \n",
    "\n",
    "As a simple example, let’s mark our linesWithSpark dataset to be cached and then invoke the first count operation to tell Spark to cache it. Remember that transformation operations such as cache does not get processed until some action like count() is called. Once you run the second count() operation, you should notice a small increase in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"Spark\" in line)\n",
    "print linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import Timer\n",
    "def count():\n",
    "    return linesWithSpark.count()\n",
    "t = Timer(lambda: count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.07555699348\n"
     ]
    }
   ],
   "source": [
    "print t.timeit(number=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63598179817\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark.cache()\n",
    "print t.timeit(number=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem silly to cache such a small file, but for larger data sets across tens or hundreds of nodes, this would still work. The second linesWithSpark.count() action runs against the cache and would perform significantly better for large datasets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
