{"cells":[{"cell_type":"markdown","source":["<h1 align=\"center\"> DataFrames</h1>\n\n\n# Table of Contents\n\n\n- [I. Catalyst Optimizer refresh](#Catalyst-Optimizer-refresh)\n\n- [II. Speeding up PySpark with DataFrames](#Speeding-up-PysPark-with-DataFrames)\n\n- [III. Creating DataFrames](#Creating-DataFrames)\n\n- [IV. Simple DataFrame queries](#Simple-DataFrame-queries)\n\n- [V. Interoperating with RDDs](#Interoperating-with-RDDs)\n\n- [VI. Querying with the DataFrame API](#Querying-with-the-DataFrame-API)\n\n- [VII. Querying with SQL](#Querying-with-SQL)\n\n- [VIII. DataFrame scenario - on-time flight performance](#DataFrame-scenario-on-time-flight-performance)\n\n- [IX. Spark Dataset API](#Spark-Dataset-API)"],"metadata":{}},{"cell_type":"markdown","source":["A DataFrame is an immutable distributed collection of data that is organized into named columns analogous to a table in a relational database. \n\n\n## Python to RDD communications\n\nWhenever a PySpark program is executed using RDDs, there is a potentially large overhead to execute the job. As noted in the following diagram, in the PySpark driver, the Spark Context uses Py4j to launch a JVM using the JavaSparkContext. Any RDD transformations are initially mapped to PythonRDD objects in Java.\n\nOnce these tasks are pushed out to the Spark Worker(s), PythonRDD objects launch Python subprocesses using pipes to send both code and data to be processed within Python:\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_01.jpg)\n\n\nWhile this approach allows PySpark to distribute the processing of the data to multiple Python subprocesses on multiple workers, as you can see, there is a lot of context switching and communications overhead between Python and the JVM.\n\n\n\n# Catalyst Optimizer refresh\n\none of the primary reasons the Spark SQL engine is so fast is because of the Catalyst Optimizer.This diagram looks similar to the logical/physical planner and cost model/cost-based optimization of a relational database management system (RDBMS):\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_02.jpg)\n\nThe significance of this is that, as opposed to immediately processing the query, the Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and has a cost optimizer that determines the most efficient physical plan generated."],"metadata":{}},{"cell_type":"markdown","source":["# Speeding up PysPark with DataFrames\n[back to top](#Table-of-Contents)\n\nThe significance of DataFrames and the Catalyst Optimizer (and Project Tungsten) is the increase in performance of PySpark queries when compared to non-optimized RDD queries. As shown in the following figure, prior to the introduction of DataFrames, Python query speeds were often twice as slow as the same Scala queries using RDD. Typically, this slowdown in query performance was due to the communications overhead between Python and the JVM:\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_03.jpg)\n\n*Source: Introducing DataFrames in Apache-spark for Large Scale Data Science at http://bit.ly/2blDBI1*\n\n\nPython can take advantage of the performance optimizations in Spark even while the codebase for the Catalyst Optimizer is written in Scala. Basically, it is a Python wrapper of approximately 2,000 lines of code that allows PySpark DataFrame queries to be significantly faster.Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R DataFrames) are all able to make use of the Catalyst Optimizer (as per the following updated diagram):\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_04.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["# Creating DataFrames\n[back to top](#Table-of-Contents)\n\n## Generate our own DataFrame\n\nInstead of accessing the file system, let's create a DataFrame by generating the data. In this case, we'll first create the ```stringRDD RDD``` and then convert it into a DataFrame when we're reading ```stringJSONRDD``` using ```spark.read.json```."],"metadata":{}},{"cell_type":"code","source":["# Generate our own JSON data \n#   This way we don't have to access the file system yet.\nstringJSONRDD = sc.parallelize((\"\"\" \n  { \"id\": \"123\",\n    \"name\": \"Katie\",\n    \"age\": 19,\n    \"eyeColor\": \"brown\"\n  }\"\"\",\n   \"\"\"{\n    \"id\": \"234\",\n    \"name\": \"Michael\",\n    \"age\": 22,\n    \"eyeColor\": \"green\"\n  }\"\"\", \n  \"\"\"{\n    \"id\": \"345\",\n    \"name\": \"Simone\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\"\n  }\"\"\")\n)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# create DataFrame\nswimmersJSON = spark.read.json(stringJSONRDD)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Create temporary table\nswimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# SQL Qeury\nspark.sql(\"select * from swimmersJSON\").collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Below is the DAG visualization for the job above.\n![](https://i.imgur.com/Zpe3pOL.png)"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Query Data\nselect * from swimmersJSON"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["As you can see from above, we can programmatically apply the ```schema``` instead of allowing the Spark engine to infer the schema via reflection.\n\nAdditional Resources include:\n\n- [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a CSV file.\n\n\n### || SparkSession\n\nWe're no longer using ```sqlContext.read```... but instead ```spark.read```.... This is because as part of Spark 2.0, ```HiveContext, SQLContext, StreamingContext, SparkContext``` have been merged together into the Spark Session spark.\n\n- Entry point for reading data\n- Working with metadata\n- Configuration\n- Cluster resource management\n\nFor more information, please refer to How to use SparkSession in Apache Spark 2.0 (http://bit.ly/2br0Fr1)."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"03-DataFrames","notebookId":3009873234848024},"nbformat":4,"nbformat_minor":0}
