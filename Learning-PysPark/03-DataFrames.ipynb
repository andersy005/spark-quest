{"cells":[{"cell_type":"markdown","source":["<h1 align=\"center\"> DataFrames</h1>\n\n\n# Table of Contents\n\n\n- [I. Catalyst Optimizer refresh](#Catalyst-Optimizer-refresh)\n\n- [II. Speeding up PySpark with DataFrames](#Speeding-up-PysPark-with-DataFrames)\n\n- [III. Creating DataFrames](#Creating-DataFrames)\n\n- [IV. Simple DataFrame queries](#Simple-DataFrame-queries)\n\n- [V. Interoperating with RDDs](#Interoperating-with-RDDs)\n\n- [VI. Querying with the DataFrame API](#Querying-with-the-DataFrame-API)\n\n- [VII. Querying with SQL](#Querying-with-SQL)\n\n- [VIII. DataFrame scenario - on-time flight performance](#DataFrame-scenario-on-time-flight-performance)\n\n- [IX. Spark Dataset API](#Spark-Dataset-API)"],"metadata":{}},{"cell_type":"markdown","source":["A DataFrame is an immutable distributed collection of data that is organized into named columns analogous to a table in a relational database. \n\n\n## Python to RDD communications\n\nWhenever a PySpark program is executed using RDDs, there is a potentially large overhead to execute the job. As noted in the following diagram, in the PySpark driver, the Spark Context uses Py4j to launch a JVM using the JavaSparkContext. Any RDD transformations are initially mapped to PythonRDD objects in Java.\n\nOnce these tasks are pushed out to the Spark Worker(s), PythonRDD objects launch Python subprocesses using pipes to send both code and data to be processed within Python:\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_01.jpg)\n\n\nWhile this approach allows PySpark to distribute the processing of the data to multiple Python subprocesses on multiple workers, as you can see, there is a lot of context switching and communications overhead between Python and the JVM.\n\n\n\n# Catalyst Optimizer refresh\n\none of the primary reasons the Spark SQL engine is so fast is because of the Catalyst Optimizer.This diagram looks similar to the logical/physical planner and cost model/cost-based optimization of a relational database management system (RDBMS):\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_02.jpg)\n\nThe significance of this is that, as opposed to immediately processing the query, the Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and has a cost optimizer that determines the most efficient physical plan generated."],"metadata":{}},{"cell_type":"markdown","source":["# Speeding up PysPark with DataFrames\n[back to top](#Table-of-Contents)\n\nThe significance of DataFrames and the Catalyst Optimizer (and Project Tungsten) is the increase in performance of PySpark queries when compared to non-optimized RDD queries. As shown in the following figure, prior to the introduction of DataFrames, Python query speeds were often twice as slow as the same Scala queries using RDD. Typically, this slowdown in query performance was due to the communications overhead between Python and the JVM:\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_03.jpg)\n\n*Source: Introducing DataFrames in Apache-spark for Large Scale Data Science at http://bit.ly/2blDBI1*\n\n\nPython can take advantage of the performance optimizations in Spark even while the codebase for the Catalyst Optimizer is written in Scala. Basically, it is a Python wrapper of approximately 2,000 lines of code that allows PySpark DataFrame queries to be significantly faster.Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R DataFrames) are all able to make use of the Catalyst Optimizer (as per the following updated diagram):\n\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_03_04.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["# Creating DataFrames\n[back to top](#Table-of-Contents)\n\n## Generate our own DataFrame\n\nInstead of accessing the file system, let's create a DataFrame by generating the data. In this case, we'll first create the ```stringRDD RDD``` and then convert it into a DataFrame when we're reading ```stringJSONRDD``` using ```spark.read.json```."],"metadata":{}},{"cell_type":"code","source":["# Generate our own JSON data \n#   This way we don't have to access the file system yet.\nstringJSONRDD = sc.parallelize((\"\"\" \n  { \"id\": \"123\",\n    \"name\": \"Katie\",\n    \"age\": 19,\n    \"eyeColor\": \"brown\"\n  }\"\"\",\n   \"\"\"{\n    \"id\": \"234\",\n    \"name\": \"Michael\",\n    \"age\": 22,\n    \"eyeColor\": \"green\"\n  }\"\"\", \n  \"\"\"{\n    \"id\": \"345\",\n    \"name\": \"Simone\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\"\n  }\"\"\")\n)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# create DataFrame\nswimmersJSON = spark.read.json(stringJSONRDD)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["![Imgur](http://i.imgur.com/PmQrksQ.png)"],"metadata":{}},{"cell_type":"code","source":["# Create temporary table\nswimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Simple DataFrame queries\n[back to top](#Table-of-Contents)\n\nNow that we have created the swimmersJSON DataFrame, we will be able to run the DataFrame API, as well as SQL queries against it. Let's start with a simple query showing all the rows within the DataFrame.\n\n\n## DataFrame API query\n\nTo do this using the DataFrame API, we can use the ```show(<n>)``` method, which prints the first n rows to the console:"],"metadata":{}},{"cell_type":"code","source":["# DataFrame API\nswimmersJSON.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## SQL query"],"metadata":{}},{"cell_type":"code","source":["# SQL Qeury\nspark.sql(\"select * from swimmersJSON\").collect()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Below is the DAG visualization for the job above.\n![](https://i.imgur.com/Zpe3pOL.png)"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Query Data\nselect * from swimmersJSON"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["# Interoperating with RDDs\n\nThere are two different methods for converting existing RDDs to DataFrames (or Datasets[T]): \n- inferring the schema using reflection, or \n- programmatically specifying the schema. \n\nThe former allows you to write more concise code (when your Spark application already knows the schema), while the latter allows you to construct DataFrames when the columns and their data types are only revealed at run time. \n\n**Note:** reflection is in reference to schema reflection as opposed to Python reflection."],"metadata":{}},{"cell_type":"markdown","source":["### Inferring the Schema using Reflection\n\nIn the process of building the DataFrame and running the queries, we skipped over the fact that the schema for this DataFrame was automatically defined. Initially, row objects are constructed by passing a list of key/value pairs as ```**kwargs``` to the row class. Then, Spark SQL converts this RDD of row objects into a DataFrame, where the keys are the columns and the data types are inferred by sampling the data.\n\nApache Spark is inferring the schema using reflection; i.e it automatically determines the schema of the data based on reviewing the JSON data."],"metadata":{}},{"cell_type":"code","source":["# Print the schema\nswimmersJSON.printSchema()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Notice that Spark was able to determine infer the schema (when reviewing the schema using .printSchema).\nBut what if we want to programmatically specify the schema?\n\n### Programmatically Specifying the Schema \n\nIn this case, let's specify the schema for a CSV text file."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Generate our own CSV data\nstringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), \n                               (234, 'Michael', 22, 'green'), \n                               (345, 'Simone', 23, 'blue')\n                              ])\n\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["First, we will encode the schema as a string, per the ```[schema]``` variable below. Then we will define the schema using ```StructType``` and ```StructField```:"],"metadata":{}},{"cell_type":"code","source":["\n# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\nschemaString = \"id name age eyeColor\"\nschema = StructType([\n    StructField(\"id\", LongType(), True),    \n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", LongType(), True),\n    StructField(\"eyeColor\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Note, the ```StructField``` class is broken down in terms of:\n- ```name```: The name of this field\n- ```dataType```: The data type of this field\n- ```nullable```: Indicates whether values of this field can be null\n\nFinally, we will apply the schema (```schema```) we created to the ```stringCSVRDD RDD``` (that is, the generated```.csv``` data) and create a temporary view so we can query it using SQL:"],"metadata":{}},{"cell_type":"code","source":["\n# Apply the schema to the RDD and Create DataFrame\nswimmers = spark.createDataFrame(stringCSVRDD, schema)\n\n\n# Creates a temporary view using the DataFrame\nswimmers.createOrReplaceTempView(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["With this example, we have finer-grain control over the schema and can specify that ```id``` is a ```long``` (as opposed to a string in the previous section):"],"metadata":{}},{"cell_type":"code","source":["# Print the schema\n# we have redefined id as Long (instead of String)\nswimmers.printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql \n-- Query the data\nselect * from swimmers"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["As you can see from above, we can programmatically apply the ```schema``` instead of allowing the Spark engine to infer the schema via reflection.\n\nAdditional Resources include:\n\n- [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a CSV file.\n\n\n### || SparkSession\n\nWe're no longer using ```sqlContext.read```... but instead ```spark.read```.... This is because as part of Spark 2.0, ```HiveContext, SQLContext, StreamingContext, SparkContext``` have been merged together into the Spark Session spark.\n\n- Entry point for reading data\n- Working with metadata\n- Configuration\n- Cluster resource management\n\nFor more information, please refer to How to use SparkSession in Apache Spark 2.0 (http://bit.ly/2br0Fr1)."],"metadata":{}},{"cell_type":"markdown","source":["# Querying with the DataFrame API\nwe can start off by using ```collect(), show(), or take()``` to view the data within our DataFrame (with the last two including the option to limit the number of returned rows).\n\n## Number of rows\nTo get the number of rows within our DataFrame, we can use the ```count()``` method:"],"metadata":{}},{"cell_type":"code","source":["swimmers.count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Running filter statements\n\nTo run a filter statement, you can use the ```filter``` clause; in the following code snippet, we are using the ```select``` clause to specify the columns to be returned as well:"],"metadata":{}},{"cell_type":"code","source":["# Get the id, age where age = 22\nswimmers.select(\"id\", \"age\").filter(\"age = 22\").show()\n\n# or \nswimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["![Imgur](http://i.imgur.com/o3GmKQD.png)"],"metadata":{}},{"cell_type":"code","source":["# Get the name, eyecolor where eyecolor like 'b%'\nswimmers.select(\"name\", \"eyecolor\").filter(\"eyecolor like 'b%'\").show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["![Imgur](http://i.imgur.com/WdBxuXF.png)"],"metadata":{}},{"cell_type":"markdown","source":["# On-Time Flight Performance\n\nQuerying flight departure delays by State and City by joining the departure delay to the airport codes(to identify state and city).\n\n\n## DataFrame Queries\nLet's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets.\n\n## Preparing the source datasets\n\nWe will first process the source airports and flight performance datasets by specifying their file path location and importing them using SparkSession:"],"metadata":{}},{"cell_type":"code","source":["# Set File Paths\nflightPerfFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n\n# Obtain Airports dataset\nairports = spark.read.csv(airportsFilePath, header=\"true\", \n                          inferSchema=\"true\", sep=\"\\t\")\nairports.createOrReplaceTempView(\"airports\")\n\n# Obtain Departure Delays dataset\nflightPerf = spark.read.csv(flightPerfFilePath, header=\"true\")\nflightPerf.createOrReplaceTempView(\"FlightPerformance\")\n\n\n# Cache the Departure Delays dataset\nflightPerf.cache()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## Joining flight performance and airports\n\nOne of the more common tasks with DataFrames/SQL is to join two different datasets; it is often one of the more demanding operations (from a performance perspective). With DataFrames, a lot of the performance optimizations for these joins are included by default:"],"metadata":{}},{"cell_type":"code","source":["# Query sum of flight Delays by City and Origin Code (for washington State)'\nspark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["n our scenario, we are querying the total delays by city and origin code for the state of Washington. This will require joining the flight performance data with the airports data by International Air Transport Association (IATA) code. The output of the query is as follows:\n\n![](https://i.imgur.com/WZbV1sy.png)"],"metadata":{}},{"cell_type":"markdown","source":["![](https://i.imgur.com/dTd0cgZ.png)"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\nselect a.City, f.origin, sum(f.delay) as Delays\n from FlightPerformance f\n    join airports a\n      on a.IATA = f.origin\n        \nwhere a.State = 'WA'\ngroup by a.City, f.origin\norder by sum(f.delay) desc"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## Visualizing our flight-performance data\n\nLet's continue visualizing our data, but broken down by all states in the continental US:"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by State (for the US)\nselect a.State, sum(f.delay) as Delays  \n  from FlightPerformance f    \n    join airports a      \n      on a.IATA = f.origin \nwhere a.Country = 'USA'\ngroup by a.State"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["![](https://i.imgur.com/7uxbRfu.png)\n\n![](https://i.imgur.com/wn06vwB.png)"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by State (for the US)\nselect a.State, sum(f.delay) as Delays  \n  from FlightPerformance f    \n    join airports a      \n      on a.IATA = f.origin \nwhere a.Country = 'Canada'\ngroup by a.State"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql\n-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\nselect a.City, f.origin, sum(f.delay) as Delays\n from FlightPerformance f\n    join airports a\n      on a.IATA = f.origin\n        \nwhere a.State = 'IL'\ngroup by a.City, f.origin\norder by sum(f.delay) desc"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47}],"metadata":{"name":"03-DataFrames","notebookId":3009873234848024},"nbformat":4,"nbformat_minor":0}
