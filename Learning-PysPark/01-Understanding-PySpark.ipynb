{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [I. What is Apache Spark?](#What-is-Apache-Spark?)\n",
    "- [II. Spark Jobs and APIs](#Spark-Jobs-and-APIs)\n",
    "- [III. RDDs, DataFrames, and Datasets](#RDDs-DataFrames-Datasets)\n",
    "- [IV. Catalyst Optimizer](#Catalyst-Optimizer)\n",
    "\n",
    "  \n",
    "\n",
    "# What is Apache Spark?\n",
    "\n",
    "Apache Spark is:\n",
    "- an open-source\n",
    "- powerful\n",
    "- distributed\n",
    "- querying and\n",
    "- processing engine\n",
    "\n",
    "It provides:\n",
    "- flexibility\n",
    "- extensibility of MapReduce\n",
    "but at significantly higher speeds.\n",
    "\n",
    "Apache Spark allows the user to:\n",
    "- read\n",
    "- transform\n",
    "- and aggregate data\n",
    "- as well as train\n",
    "- deploy sophisticated statistical models\n",
    "\n",
    "\n",
    "The Spark APIs are accessible in \n",
    "- Java\n",
    "- Scala\n",
    "- Python\n",
    "- R \n",
    "- SQL\n",
    "\n",
    "Apache Spark can be used to:\n",
    "- build applications\n",
    "- package them up as libraries to be deployed on a cluster\n",
    "- perform quick analytics interactively through notebooks:\n",
    " - Jupyter\n",
    " - Spark-Notebook\n",
    " - Databricks notebooks\n",
    " - Apache Zeppelin\n",
    " \n",
    "Apache Spark exposes a host of libraries familiar to data analysts, data scientists or researchers who have worked with Python's ```pandas``` or R's ```data.frames``` or ```data.tables```.\n",
    "\n",
    "Note: There are some differences between pandas or data.frames/data.tables and Spark DataFrames.\n",
    "\n",
    "Also, delivered with Apache Spark are several already implemented and tuned algorithms, statistical models, and frameworks: MLlib and ML for machine learning, GraphX and GraphFrames for graph processing, and Spark Streaming (DStreams and Structured). Spark allows the user to combine these libraries seamlessly in the same application.\n",
    "\n",
    "Apache Spark can easily run locally on a laptop, yet can also easily be deployed in standalone mode, over YARN, or Apache Mesos - either on your local cluster or in the cloud. It can read and write from a diverse data sources including (but not limited to) HDFS, Apache Cassandra, Apache HBase, and S3:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_01.jpg)\n",
    "\n",
    "*Source: Apache Spark is the smartphone of Big Data http://bit.ly/1QsgaNj*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark Jobs and APIs\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Execution process\n",
    "\n",
    "Any Spark application spins off a single driver process(that can contain multiple jobs) on the **master node** that then directs **executor** processes(that contain multiple tasks) distributed to a number of **worker nodes**\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_02.jpg)\n",
    "\n",
    "The driver process determines the number and the composition of the task processes directed to the executor nodes based on the graph generated for the given job.\n",
    "\n",
    "Note: Any worker node can execute tasks from a number of different jobs.\n",
    "\n",
    "\n",
    "A Spark job is associated with a chain of object dependencies organized in a **direct acyclic graph(DAG)** such as the following example generated from the Spark UI. Given this, Spark Can optimize the scheduling ( for example, determine the number of tasks and workers required) and execution of these tasks:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs, DataFrames, and Datasets\n",
    "\n",
    "## Resilient Distributed Dataset\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "Spark is built around a distributed collection of immutable Java Virtual Machine(JVM) objects called **Resilient Distributed Datasets(RDDs)**.\n",
    "\n",
    "In PySpark, it is important to note that the Python data is stored within these JVM objects and these objects allow  any job to perform calculations very quickly.\n",
    "\n",
    "RDDs are:\n",
    "- calculated against\n",
    "- cached\n",
    "- stored in-memory\n",
    "\n",
    "At the same time, RDDs expose some coarse-gained transformations such as:\n",
    "- ```map(...)```\n",
    "- ```reduce(...)```\n",
    "- ```filter(...)```\n",
    "\n",
    "RDDs have two sets of parallel operations:\n",
    "- **transformations**(which return pointers to new RDDs) and\n",
    "- **actions**(which return values to the driver after running a computation)\n",
    "\n",
    "\n",
    "RDD transformation operations are lazy in a sense that they do not compute their results immediately. The transformations are only computed when an action is executed and the results need to be returned to the driver. This delayed execution results in more fine-tuned queries: Queries that are optimized for performance. \n",
    "\n",
    "## DataFrames\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "DataFrames, like RDDs, are immutable collections of data distributed among teh nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into named columns.\n",
    "\n",
    "\n",
    "DataFrames were designed to make large data sets processing even easier. They allow developers to formalize the structure of the data, allowing higher-level abstraction; in that sense DataFrames resemble tables from the relational database world. DataFrames provide a domain specific language API to manipulate the distributed data and make Spark accessible to a wider audience, beyond specialized data engineers.\n",
    "\n",
    "One of the major benefits of DataFrames is that the Spark Engine initially builds a logical execution plan and executes generated code based on a physical plan determined by a cost optimizer. Unlide RDDs that can be significantly slower on Python compared with Java or Scala.\n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "The goal of Spark Datasets is to provide an API that allows users to easily express transformations on domain objects, while also providing the performance and benefits of the robust Spark SQL execution engine. \n",
    "\n",
    "\n",
    "\n",
    "# Catalyst Optimizer\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "Spark SQL is one of the most technically involved components of Apache Spark as it powers both SQL queries and the DataFrame API. At the core of Spark SQL is the Catalyst Optimizer. The optimizer is based on functional programming constructs and was designed with two purposes in mind: \n",
    "- To ease the addition of new optimization techniques and features to Spark SQL and \n",
    "- to allow external developers to extend the optimizer (for example, adding data source specific rules, support for new data types, and so on):\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_04.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
