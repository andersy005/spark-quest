{"cells":[{"cell_type":"markdown","source":["<h1 align=\"center\"> Prepare Data for Modeling</h1>"],"metadata":{}},{"cell_type":"markdown","source":["# Table of Contents\n\n- [Duplicates](#Duplicates)\n- [Missing Observations](#Missing-Observations)\n- [Outliers](#Outliers)"],"metadata":{}},{"cell_type":"markdown","source":["All data is dirty, irrespective of what the source of the data might lead you to believe: it might be your colleague, a telemetry system that monitors your environment, a dataset you download from the web, or some other source. Until you have tested and proven to yourself that your data is in a clean state, you should neither trust it nor use it for modeling.\n\nYour data can be stained with duplicates, missing observations and outliers, non-existent addresses, wrong phone numbers and area codes, inaccurate geographical coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases, trailing spaces, and many other more subtle problems. It is your job to clean it, irrespective of whether you are a data scientist or data engineer, so you can build a statistical or machine learning model.\n\n\nYour dataset is considered technically clean if none of the aforementioned problems can be found. However, to clean the dataset for modeling purposes, you also need to check the distributions of your features and confirm they fit the predefined criteria.\n\nAs a data scientist, you can expect to spend 80-90% of your time **massaging** your data and getting familiar with all the features. This chapter will guide you through that process, leveraging Spark capabilities."],"metadata":{}},{"cell_type":"markdown","source":["# Duplicates\n[back to top](#Table-of-Contents)\n\nDuplicates are observations that appear as distinct rows in your dataset, but which, upon closer inspection, look the same. That is, if you looked at them side by side, all the features in these two (or more) rows would have exactly the same values.\n\nOn the other hand, if your data has some form of an ID to distinguish between records (or associate them with certain users, for example), then what might initially appear as a duplicate may not be; sometimes systems fail and produce erroneous IDs. In such a situation, you need to either check whether the same ID is a real duplicate, or you need to come up with a new ID system."],"metadata":{}},{"cell_type":"code","source":["df = spark.createDataFrame([\n        (1, 144.5, 5.9, 33, 'M'),\n        (2, 167.2, 5.4, 45, 'M'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (4, 144.5, 5.9, 33, 'M'),\n        (5, 133.2, 5.7, 54, 'F'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (5, 129.2, 5.3, 42, 'M'),\n    ], ['id', 'weight', 'height', 'age', 'gender'])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check for duplicates\nprint('Count of rows: {0}'.format(df.count()))\nprint('Count of distinct rows: {0}'.format(df.distinct().count()))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We can drop these duplicate rows by using the .dropDuplicates(...) method."],"metadata":{}},{"cell_type":"code","source":["df = df.dropDuplicates()\ndf.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Let's confirm\nprint('Count of ids: {0}'.format(df.count()))\nprint('Count of distinct ids: {0}'.format(df.select([c for c in df.columns if c != 'id']).distinct().count()))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We still have one more duplicate. We use the ```.dropDuplicates(...)``` but add the ```subset``` parameter\n\nThe subset parameter instructs the ```.dropDuplicates(...)``` method to look for duplicated rows using only the columns specified via the subset parameter; in the preceding example, we will drop the duplicated records with the same weight, height, age, and gender but not id."],"metadata":{}},{"cell_type":"code","source":["df = df.dropDuplicates(subset = [c for c in df.columns if c!= 'id'])\ndf.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["To calculate the total and distinct number of IDs in one step we can use the ```.agg(...)``` method."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as fn\n\ndf.agg(\n       fn.count('id').alias('count'),\n       fn.countDistinct('id').alias('distinct')\n).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["we use the ```.count(...)``` and ```.countDistinct(...)``` to, respectively, calculate the number of rows and the number of distinct ids in our DataFrame. The .alias(...) method allows us to specify a friendly name to the returned column.\n\nAs you can see, we have five rows in total, but only four distinct IDs. Since we have already dropped all the duplicates, we can safely assume that this might just be a fluke in our ID data, so we will give each row a unique ID:"],"metadata":{}},{"cell_type":"code","source":["# Give each row a unique ID.\ndf.withColumn('new_id', fn.monotonically_increasing_id()).show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["# Missing Observations\n[back to top](#Table-of-Contents)\n\n\nThe missing values can happen for a variety of reasons: systems failure, people error, data schema changes, just to name a few.\n\nThe simplest way to deal with missing values, if your data can afford it, is to drop the whole observation when any missing value is found. You have to be careful not to drop too many: depending on the distribution of the missing values across your dataset it might severely affect the usability of your dataset. If, after dropping the rows, I end up with a very small dataset, or find that the reduction in data size is more than 50%, I start checking my data to see what features have the most holes in them and perhaps exclude those altogether; if a feature has most of its values missing (unless a missing value bears a meaning), from a modeling point of view, it is fairly useless.\n\nThe other way to deal with the observations with missing values is to impute some value in place of those Nones. Given the type of your data, you have several options to choose from:\n\n\n\n- If your data is a discrete Boolean, you can turn it into a categorical variable by adding a third category â€” Missing\n- If your data is already categorical, you can simply extend the number of levels and add the Missing category as well\n- If you're dealing with ordinal or numerical data, you can impute either mean, median, or some other predefined value (for example, first or third quartile, depending on the distribution shape of your data)"],"metadata":{}},{"cell_type":"code","source":["df_miss = spark.createDataFrame([\n        (1, 143.5, 5.6, 28,   'M',  100000),\n        (2, 167.2, 5.4, 45,   'M',  None),\n        (3, None , 5.2, None, None, None),\n        (4, 144.5, 5.9, 33,   'M',  None),\n        (5, 133.2, 5.7, 54,   'F',  None),\n        (6, 124.1, 5.2, None, 'F',  None),\n        (7, 129.2, 5.3, 42,   'M',  76000),\n    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["To find the number of missing observations per row we can use the following snippet."],"metadata":{}},{"cell_type":"code","source":["df_miss.rdd.map(\n             lambda row: (row['id'], sum([c == None for c in row]))).collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["To see what values are missing, we count missing observations in columns we can decide to drop the observation altogether or impute some of the observations."],"metadata":{}},{"cell_type":"code","source":["df_miss.where('id == 3').show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Here is what we get:\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_04_08.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["Let's now check what percentage of missing observations are there in each column:"],"metadata":{}},{"cell_type":"code","source":["df_miss.agg(*[\n    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n    for c in df_miss.columns\n  ]).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["This is what we get\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_04_09.jpg)\n\n**Note:**\n\nThe ```*``` argument to the ```.count(...)``` method (in place of a column name) instructs the method to count all rows. On the other hand, the ```*``` preceding the list declaration instructs the ```.agg(...)``` method to treat the list as a set of separate parameters passed to the function.\n\nSo, we have 14% of missing observations in the weight and gender columns, twice as much in the height column, and almost 72% of missing observations in the income column. Now we know what to do.First, we will drop the 'income' feature, as most of its values are missing."],"metadata":{}},{"cell_type":"code","source":["df_miss_no_income = df_miss.select([\n    c for c in df_miss.columns if c != 'income'\n  ])\ndf_miss_no_income.show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["if you decide to drop the observations instead, you can use the ```.dropna(...)``` method, as shown here. Here, we will also use the thresh parameter, which allows us to specify a threshold on the number of missing observations per row that would qualify the row to be dropped. This is useful if you have a dataset with tens or hundreds of features and you only want to drop those rows that exceed a certain threshold of missing values:"],"metadata":{}},{"cell_type":"code","source":["df_miss_no_income.dropna(thresh=3).show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Here is the output\n![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_04_10.jpg)\n\nOn the other hand, if you wanted to impute the observations, you can use the .fillna(...) method. This method accepts a single integer (long is also accepted), float, or string; all missing values in the whole dataset will then be filled in with that value. You can also pass a dictionary of a form ```{'<colName>': <value_to_impute>}```. This has the same limitation, in that, as the ```<value_to_impute>```, you can only pass an integer, float, or string.\n\nIf you want to impute a mean, median, or other calculated value, you need to first calculate the value, create a dictionary with such values, and then pass it to the .fillna(...) method."],"metadata":{}},{"cell_type":"code","source":["means = df_miss_no_income.agg(\n          *[fn.mean(c).alias(c)\n           for c in df_miss_no_income.columns if c != 'gender']\n          ).toPandas().to_dict('records')[0]\n\n\nmeans['gender'] = 'missing'\n\ndf_miss_no_income.fillna(means).show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_04_11.jpg)\n\n**Note that** calling the .toPandas() can be problematic, as the method works essentially in the same way as .collect() in RDDs. It collects all the information from the workers and brings it over to the driver. It is unlikely to be a problem with the preceding dataset, unless you have thousands upon thousands of features."],"metadata":{}},{"cell_type":"markdown","source":["# Outliers\n\n[back to top](#Table-of-Contents)\n\n**Outliers** are those observations that deviate significantly from the distribution of the rest of your sample. The definitions of significance vary, but in the most general form, you can accept that there are no outliers if all the values are roughly within the Q1âˆ’1.5IQR and Q3+1.5IQR range, where IQR is the interquartile range; the IQR is defined as a difference between the upper- and lower-quartiles, that is, the 75th percentile (the Q3) and 25th percentile (the Q1), respectively.\n\nConsider another simple example."],"metadata":{}},{"cell_type":"code","source":["df_outliers = spark.createDataFrame([\n        (1, 143.5, 5.3, 28),\n        (2, 154.2, 5.5, 45),\n        (3, 342.3, 5.1, 99),\n        (4, 144.5, 5.5, 33),\n        (5, 133.2, 5.4, 54),\n        (6, 124.1, 5.1, 21),\n        (7, 129.2, 5.3, 42),\n    ], ['id', 'weight', 'height', 'age'])"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Now we can use the definition we outlined previously to flag the outliers.\n\nFirst, we calculate the lower and upper cut off points for each feature. We will use the .approxQuantile(...) method. The first parameter specified is the name of the column, the second parameter can be either a number between 0 or 1 (where 0.5 means to calculated median) or a list (as in our case), and the third parameter specifies the acceptable level of an error for each metric (if set to 0, it will calculate an exact value for the metric, but it can be really expensive to do so):\n\n**i.e calculate the lower and upper cut off points for each feature.**"],"metadata":{}},{"cell_type":"code","source":["cols = ['weight', 'height', 'age']\nbounds = {}\n\nfor col in cols:\n  quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.01)\n  \n  IQR = quantiles[1] - quantiles[0]\n  \n  bounds[col] = [quantiles[0] - 1.5 * IQR,\n                 quantiles[1] + 1.5 * IQR]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["The bounds dictionary holds the lower and upper bounds for each feature:"],"metadata":{}},{"cell_type":"code","source":["bounds"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Let's now use it to flag our outliers."],"metadata":{}},{"cell_type":"code","source":["outliers = df_outliers.select(*['id'] + [\n        (\n              (df_outliers[c] < bounds[c][0]) |\n              (df_outliers[c] > bounds[c][1])\n        ).alias(c + '_o') for c in cols\n  ])\n\noutliers.show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["We have two outliers in the weight feature and two in the age feature."],"metadata":{}},{"cell_type":"code","source":["df_outliers = df_outliers.join(outliers, on='id')\ndf_outliers.filter('weight_o').select('id', 'weight').show()\ndf_outliers.filter('age_o').select('id', 'age').show()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["**Equipped with the methods described in this section, you can quickly clean up even the biggest of datasets.**"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"04-Prepare-Data-for-Modeling","notebookId":836708506337496},"nbformat":4,"nbformat_minor":0}
