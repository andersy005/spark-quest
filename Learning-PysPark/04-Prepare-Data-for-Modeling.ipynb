{"cells":[{"cell_type":"markdown","source":["<h1 align=\"center\"> Prepare Data for Modeling</h1>"],"metadata":{}},{"cell_type":"markdown","source":["# Table of Contents\n\n- [Duplicates](#Duplicates)"],"metadata":{}},{"cell_type":"markdown","source":["All data is dirty, irrespective of what the source of the data might lead you to believe: it might be your colleague, a telemetry system that monitors your environment, a dataset you download from the web, or some other source. Until you have tested and proven to yourself that your data is in a clean state, you should neither trust it nor use it for modeling.\n\nYour data can be stained with duplicates, missing observations and outliers, non-existent addresses, wrong phone numbers and area codes, inaccurate geographical coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases, trailing spaces, and many other more subtle problems. It is your job to clean it, irrespective of whether you are a data scientist or data engineer, so you can build a statistical or machine learning model.\n\n\nYour dataset is considered technically clean if none of the aforementioned problems can be found. However, to clean the dataset for modeling purposes, you also need to check the distributions of your features and confirm they fit the predefined criteria.\n\nAs a data scientist, you can expect to spend 80-90% of your time **massaging** your data and getting familiar with all the features. This chapter will guide you through that process, leveraging Spark capabilities."],"metadata":{}},{"cell_type":"markdown","source":["# Duplicates\n[back to top](#Table-of-Contents)\n\nDuplicates are observations that appear as distinct rows in your dataset, but which, upon closer inspection, look the same. That is, if you looked at them side by side, all the features in these two (or more) rows would have exactly the same values.\n\nOn the other hand, if your data has some form of an ID to distinguish between records (or associate them with certain users, for example), then what might initially appear as a duplicate may not be; sometimes systems fail and produce erroneous IDs. In such a situation, you need to either check whether the same ID is a real duplicate, or you need to come up with a new ID system."],"metadata":{}},{"cell_type":"code","source":["df = spark.createDataFrame([\n        (1, 144.5, 5.9, 33, 'M'),\n        (2, 167.2, 5.4, 45, 'M'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (4, 144.5, 5.9, 33, 'M'),\n        (5, 133.2, 5.7, 54, 'F'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (5, 129.2, 5.3, 42, 'M'),\n    ], ['id', 'weight', 'height', 'age', 'gender'])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check for duplicates\nprint('Count of rows: {0}'.format(df.count()))\nprint('Count of distinct rows: {0}'.format(df.distinct().count()))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We can drop these duplicate rows by using the .dropDuplicates(...) method."],"metadata":{}},{"cell_type":"code","source":["df = df.dropDuplicates()\ndf.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Let's confirm\nprint('Count of ids: {0}'.format(df.count()))\nprint('Count of distinct ids: {0}'.format(df.select([c for c in df.columns if c != 'id']).distinct().count()))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We still have one more duplicate. We use the ```.dropDuplicates(...)``` but add the ```subset``` parameter\n\nThe subset parameter instructs the ```.dropDuplicates(...)``` method to look for duplicated rows using only the columns specified via the subset parameter; in the preceding example, we will drop the duplicated records with the same weight, height, age, and gender but not id."],"metadata":{}},{"cell_type":"code","source":["df = df.dropDuplicates(subset = [c for c in df.columns if c!= 'id'])\ndf.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["To calculate the total and distinct number of IDs in one step we can use the ```.agg(...)``` method."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as fn\n\ndf.agg(\n       fn.count('id').alias('count'),\n       fn.countDistinct('id').alias('distinct')\n).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["we use the ```.count(...)``` and ```.countDistinct(...)``` to, respectively, calculate the number of rows and the number of distinct ids in our DataFrame. The .alias(...) method allows us to specify a friendly name to the returned column.\n\nAs you can see, we have five rows in total, but only four distinct IDs. Since we have already dropped all the duplicates, we can safely assume that this might just be a fluke in our ID data, so we will give each row a unique ID:"],"metadata":{}},{"cell_type":"code","source":["# Give each row a unique ID.\ndf.withColumn('new_id', fn.monotonically_increasing_id()).show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"04-Prepare-Data-for-Modeling","notebookId":836708506337496},"nbformat":4,"nbformat_minor":0}
