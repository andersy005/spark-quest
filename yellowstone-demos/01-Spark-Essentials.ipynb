{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Anderson Banihirwe](andersy005.github.io) as part of [2017 CISL/SIParCS Research Project](https://github.com/NCAR/PySpark4Climate): **PySpark for Big Atmospheric & Oceanic Data Analysis**</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png) \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mpirun.lsf hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!bjobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To confirm that PySpark is running, run the cells below. If everything is well setup, you shouldn't get any error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [I. PySpark](#1.-PySpark)\n",
    "- [II. Resilient Distributed Datasets](#2.-Resilient-Distributed-Datasets)\n",
    "- [III. Creating an RDD](#3.-Creating-an-RDD)\n",
    "- [IV. Transformations](#4.-Transformations)\n",
    "- [V. Actions](#5.-Actions)\n",
    "- [VI. Caching RDDs](#6.-Caching-RDDs)\n",
    "- [VII. Spark Program Lifecycle](#7.-Spark-Program-Lifecycle)\n",
    "- [VIII. PySpark Closures](#8.-PySpark-Closures)\n",
    "- [IX. Summary](#9.-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PySpark\n",
    "\n",
    "PySpark is the python programming interface to Spark.\n",
    "\n",
    "PySpark provides an easy-to-use programming abstraction and parallel runtime:\n",
    "> Here's an operation, run it on all the data.\n",
    "\n",
    "Resilient Distributed Datasets are the key concept\n",
    "\n",
    "## 1.1. Spark Driver and Workers\n",
    "- A spark program is two programs:\n",
    "    - **A driver program**\n",
    "    - **A worker program**\n",
    "\n",
    "- Worker programs run on cluster nodes or in local threads\n",
    "- RDDs are distributed across workers\n",
    "\n",
    "![](https://i.imgur.com/HJ9gpwd.jpg)\n",
    "source: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark\n",
    "\n",
    "## 1.2. Spark Context\n",
    "- A spark program first creates a **SparkContext** object\n",
    " - The SparkContext tells Spark how and where to access a cluster.\n",
    " - PySpark Shell automatically creates the **sc** variable\n",
    " - **Jupyter notebook** and programs must use a constructor to create a new **SparkContext**\n",
    " \n",
    "- Use **SparkContext** to create RDDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Create a new SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3. Master\n",
    "The **master** parameter for a **SparkContext** determines which type and size of cluster to use.\n",
    "\n",
    "| Master Parameter  | Description                                                                   |\n",
    "|-------------------|-------------------------------------------------------------------------------|\n",
    "| local             | run Spark locally with one worker thread (no parallelism)                     |\n",
    "| local[k]          | run Spark locally with K worker threads (ideally set to number of cores)      |\n",
    "| spark://HOST:PORT | connect to a Spark standalone cluster PORT depends on config(7077 by default) |\n",
    "| mesos://HOST:PORT | connect to a Mesos cluster; PORT depends on config(5050 by default)           |\n",
    "\n",
    "The master parameter for Spark installation running on Yellowstone is set to **Spark standalone cluster**\n",
    "\n",
    "To learn more, check out [APACHE SPARK CLUSTER MANAGERS: YARN, MESOS, OR STANDALONE?](http://www.agildata.com/apache-spark-cluster-managers-yarn-mesos-or-standalone/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resilient Distributed Datasets\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "- The primary abstraction in Spark\n",
    "    - Immutable once constructed\n",
    "    - Spark tracks lineage information to efficiently recompute lost data\n",
    "    - Enable operations on collection of elements in parallel\n",
    "    \n",
    "- You construct RDDs\n",
    "    - by parallelizing existing Python collections (lists)\n",
    "    - by transforming an existing RDDs\n",
    "    - from files in HDFS or any other storage system (glade in case of Yellowstone and Cheyenne)\n",
    "    \n",
    "    \n",
    "- The programmer needs to specify the number of partitions for an RDD or the default value is used if unspecified.\n",
    "\n",
    "![Partitioning](https://i.imgur.com/zaOQIQY.jpg)\n",
    "\n",
    "\n",
    "There are two types of operations on RDDs:\n",
    "- **transformations**\n",
    "- **actions**\n",
    "\n",
    "- **transformations** are lazy in a sense that they are not computed immediately\n",
    "- Transformed RDD is executed when action runs on it.\n",
    "- RDDs can be persisted(cached) in memory or disk.\n",
    "\n",
    "## 2.1 Working with RDDs\n",
    "- Create an RDD from a data source\n",
    "- Apply transformations to an RDD: ```.map(...)```\n",
    "- Apply actions to an RDD: ```.collect()```, ```.count()```\n",
    "\n",
    "![](https://i.imgur.com/iqvUJV5.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating an RDD\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "## 3.1. Creating RDDs from Python collections (lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create a list of 30 random integers less than 50\n",
    "data = np.random.randint(50, size=30)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, no computation occurs with **```sc.parallelize()```**. Spark only records how to create the RDD with four partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Creating RDDs from a file\n",
    "We can also create RDDs from HDFS, text files, Hypertable, Amazon S3, Apache Hbase, SequenceFiles, or any other Hadoop **inputFormat**, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"spark-cluster.sh\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example,\n",
    "- RDD is distributed in 4 partitions\n",
    "- Elements are lines of input\n",
    "- **lazy evaluation** means no execution happens now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transformations\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "- Create new datasets from an existing one\n",
    "- Use **lazy evaluation**: results not computed right-away instead Spark remembers set of transformations applied to base dataset.\n",
    "    - Spark optimizes the required calculations.\n",
    "    - Spark recovers from failures and slow workers.\n",
    "    \n",
    "## 4.1 Some Transformations\n",
    "\n",
    "### ```.map(func)```\n",
    "\n",
    "This method is applied to each element of the RDD:\n",
    "\n",
    "-> returns a new distributed dataset formed by passing each element of the source through a function **func**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns an RDD with each element times two\n",
    "mapped_rdd = rdd.map(lambda x: x * 2) \n",
    "mapped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.flatMap(func)```\n",
    "\n",
    "-> The ```.flatMap(func)``` method works similar to ```.map(func)``` but returns a flattened results instead of a list. So func should return a new sequence rather than a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "flatmapped_rdd = rdd.flatMap(lambda x: [x, x+5])\n",
    "flatmapped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.filter(func)```\n",
    "The ```.filter(func)``` method allows you to select elements of our dataset that fit specified criteria.\n",
    "\n",
    "-> Returns a new dataset formed by selecting those elements of the source on which func returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "filtered_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.distinct([numTasks])```\n",
    "\n",
    "-> This method returns a list of distinct values in a specified column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_rdd = rdd.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Actions\n",
    "- Cause Spark to execute recipe to transform source\n",
    "- Mechanism for getting results out of Spark.\n",
    "\n",
    "## 5.1 Some Actions\n",
    "\n",
    "\n",
    "### ```.take(.n)```\n",
    "\n",
    "-> The method returns n top rows from a single data partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want somewhat randomized records you can use ```.takeSample(n)``` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.reduce(func)```\n",
    "\n",
    "Another action that processes your data, the ```.reduce(func)``` method **reduces** the elements of an RDD using a specified method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,3,5])\n",
    "rdd1.reduce(lambda a, b: a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```count()```\n",
    "\n",
    "The ```.count()``` method counts the number of elements in the RDD.\n",
    "\n",
    "```count()``` causes spark to:\n",
    "- read data\n",
    "- sum within partitions\n",
    "- combine sums in driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.collect()```\n",
    "-> Return all the elements as an array\n",
    "\n",
    "**BIG WARNING:** make sure will fit in driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```.reduceByKey(...)```\n",
    "The ```.reduceByKey(...)``` method works in a similar way to the ```.reduce(...)``` method but performs a reduction on a key-by-key basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_key = sc.parallelize([('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),('d', 3)],4)\n",
    "data_key.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Caching RDDs\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "To avoid to reload the data, we can use ```cache()``` to our RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save don't recompute\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7. Spark Program Lifecycle\n",
    "\n",
    "1. Create RDDs from external data or **parallelize** a collection in your driver program.\n",
    "2. Lazily **transform** them into new RDDs\n",
    "3. **```cache()```** some RDDs for reuse\n",
    "4. Perform **actions** to execute parallel computation and produce results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. PySpark Closures\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "- PySpark automatically creates closures for:\n",
    "    - Functions that run on RDDs at workers.\n",
    "    - Any global variables used by workers\n",
    "    \n",
    "- One closure per worker\n",
    "    - sent for every task\n",
    "    - No communication between workers\n",
    "    - changes to global variables at workers are not sent to driver\n",
    "    \n",
    "## 8.2. Consider These Use Cases\n",
    "- Iterative or single jobs with large global variables:\n",
    "    - Sending large read-only lookup table to workers\n",
    "    - Sending large feature vector in a Machine Learning algorithms to workers\n",
    "    \n",
    "- Counting events that occur during job execution\n",
    "    - How many input lines were blank?\n",
    "    - How many inpu records were corrupt\n",
    " \n",
    "<div style=\"color:red;\">\n",
    "Problems:\n",
    "<ul>\n",
    "    <li>Closures are (re-) sent with every job</li>\n",
    "    <li>Inefficient to send large data to each worker</li>\n",
    "    <li>Closures are one way: driver -> worker</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<p style=\"color:red;\">Solution:</p>\n",
    "\n",
    "## 8.3. PySpark Shared Variables\n",
    "\n",
    "### Broadcast Variables:\n",
    "- Efficiently send large, **read-only** value to all workers.\n",
    "- Saved at workers for use in one or more Spark operations.\n",
    "- Like sending a large, read-only lookup table to all the nodes\n",
    "\n",
    "Example: efficiently give every worker a large dataset\n",
    "\n",
    "Broadcast variable are usually distributed using efficient broadcast algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At the driver\n",
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# At a worker (in code passed via a closure)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators:\n",
    "- Aggreagate values from workers back to driver\n",
    "- only driver can access value of accumulator\n",
    "- For tasks, accumulators are write-only\n",
    "- Use to count errors seen in RDD across workers\n",
    "- Variables that can only be **added** to by associative operations\n",
    "- Efficiently implement parallel counters and sums\n",
    "- only driver can read an accumulator's value, not tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "rdd2 = sc.parallelize([1, 2, 3, 4])\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2.foreach(f)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Summary\n",
    "![](https://i.imgur.com/EuyK62Q.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "References:\n",
    "1. https://spark.apache.org/docs/latest/programming-guide.html\n",
    "2. https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
