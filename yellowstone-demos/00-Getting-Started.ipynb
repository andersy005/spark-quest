{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<small><i>This notebook was put together by [Anderson Banihirwe](andersy005.github.io) as part of [2017 CISL/SIParCS Research Project](https://github.com/NCAR/PySpark4Climate): **PySpark for Big Atmospheric & Oceanic Data Analysis**</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [I. The Big Data Problem](#The-Big-Data-Problem)\n",
    "- [II. What is Apache Spark?](#What-is-Apache-Spark?)\n",
    "- [III. Spark Jobs and APIs](#Spark-Jobs-and-APIs)\n",
    "- [IV. RDDs, DataFrames, and Datasets](#RDDs,-DataFrames,-and-Datasets)\n",
    "- [V. Catalyst Optimizer](#Catalyst-Optimizer)\n",
    "- [VI. Spark 2.0 architecture](#Spark-2.0-architecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Big Data Problem\n",
    "\n",
    "- Data is growing faster than computation speeds\n",
    "- Storage is getting cheaper\n",
    "- But, stalling CPU speeds and storage bottlenecks\n",
    "\n",
    "\n",
    "## Big Data Examples\n",
    "> In 2012, when the fifth Coupled Model Intercomparison Project (CMIP5) was in production, model data post-processing required about as much time as the model runs. \n",
    "- Those runs produced a total of **170 terabytes of CESM data**, and \n",
    "- it took **15 months just to transpose that data to the required file format**.\n",
    "\n",
    "*source: [Computational researchers speed up the analysis of climate model data](https://www2.cisl.ucar.edu/news/computational-researchers-speed-analysis-climate-model-data)*\n",
    "\n",
    "- Cost of 1TB of disk: ~$35\n",
    "\n",
    "- Time to read 1 TB from disk: 3 hours (assuming the read speed is 100 MB/s)\n",
    "\n",
    "- The Big Data Problem means that a single machine can no longer process or even store all the data!\n",
    "\n",
    "- Only solution is to **distribute** data over large clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Hard About Cluster Computing?\n",
    "- How do we split work across machines?\n",
    "    - Must consider network(how fast/slow it is, how it is organized), data locality\n",
    "    - Moving data may be very expansive\n",
    "    \n",
    "- How to deal with failures?\n",
    "    - If a server fails on average every 3 years --> with 10,000 nodes, we would see 10 faults/day.\n",
    "    - Even more difficult problem to deal with: stragglers(not failed, but slow nodes).\n",
    "    - Simplest solution: Launch another task\n",
    "\n",
    "## Apache Spark Motivation:\n",
    "- Using Map Reduce for complex jobs, interactive queries and online processing involves **lots of disk I/O** which is a very expensive computational task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "  \n",
    "\n",
    "# What is Apache Spark?\n",
    "\n",
    "Apache Spark is:\n",
    "- an open-source\n",
    "- powerful\n",
    "- distributed\n",
    "- querying and\n",
    "- processing engine\n",
    "\n",
    "It provides:\n",
    "- flexibility\n",
    "- extensibility of MapReduce\n",
    "but at significantly higher speeds.\n",
    "\n",
    "Apache Spark allows the user to:\n",
    "- read\n",
    "- transform\n",
    "- and aggregate data\n",
    "- as well as train\n",
    "- deploy sophisticated statistical models\n",
    "\n",
    "\n",
    "The Spark APIs are accessible in \n",
    "- Java\n",
    "- Scala\n",
    "- Python\n",
    "- R \n",
    "- SQL\n",
    "\n",
    "Apache Spark can be used to:\n",
    "- build applications\n",
    "- package them up as libraries to be deployed on a cluster\n",
    "- perform quick analytics interactively through notebooks:\n",
    " - Jupyter\n",
    " - Spark-Notebook\n",
    " - Databricks notebooks\n",
    " - Apache Zeppelin\n",
    " \n",
    "Apache Spark exposes a host of libraries familiar to data analysts, data scientists or researchers who have worked with Python's ```pandas``` or R's ```data.frames``` or ```data.tables```.\n",
    "\n",
    "Note: There are some differences between pandas or data.frames/data.tables and Spark DataFrames.\n",
    "\n",
    "Also, delivered with Apache Spark are several already implemented and tuned algorithms, statistical models, and frameworks: MLlib and ML for machine learning, GraphX and GraphFrames for graph processing, and Spark Streaming (DStreams and Structured). Spark allows the user to combine these libraries seamlessly in the same application.\n",
    "\n",
    "Apache Spark can easily run locally on a laptop, yet can also easily be deployed in standalone mode, over YARN, or Apache Mesos - either on your local cluster or in the cloud. It can read and write from a diverse data sources including (but not limited to) HDFS, Apache Cassandra, Apache HBase, and S3:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_01.jpg)\n",
    "\n",
    "*Source: Apache Spark is the smartphone of Big Data http://bit.ly/1QsgaNj*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark Jobs and APIs\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "- Spark provides programming abstraction and parallel runtime to hide complexities of fault-tolerance and slow machines.\n",
    "\n",
    "- Basically, what a programmer has to say is:\n",
    "> Here's an operation, run it on all the data.\n",
    " - I don't care where it runs (you schedule that)\n",
    " - In fact, feel free to run it twice on different nodes.\n",
    "\n",
    "## Execution process\n",
    "\n",
    "Any Spark application spins off a single driver process(that can contain multiple jobs) on the **master node** that then directs **executor** processes(that contain multiple tasks) distributed to a number of **worker nodes**\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_02.jpg)\n",
    "\n",
    "The driver process determines the number and the composition of the task processes directed to the executor nodes based on the graph generated for the given job.\n",
    "\n",
    "Note: Any worker node can execute tasks from a number of different jobs.\n",
    "\n",
    "\n",
    "A Spark job is associated with a chain of object dependencies organized in a **direct acyclic graph(DAG)** such as the following example generated from the Spark UI. Given this, Spark Can optimize the scheduling ( for example, determine the number of tasks and workers required) and execution of these tasks:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Map Reduce Differences\n",
    "\n",
    "|                          | Hadoop  Map Reduce | Spark                             |\n",
    "|--------------------------|--------------------|-----------------------------------|\n",
    "| Storage                  | Disk only          | In-memory or  on disk             |\n",
    "| Operations               | Map and Reduce     | Map, Reduce,  Join, Sample, etc.. |\n",
    "| Execution model          | Batch              |  Batch, interactive, Streaming    |\n",
    "| Programming environments | Java               | Scala, Java, Python, and R        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs, DataFrames, and Datasets\n",
    "\n",
    "## Resilient Distributed Dataset\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "Spark is built around a distributed collection of immutable Java Virtual Machine(JVM) objects called **Resilient Distributed Datasets(RDDs)**.\n",
    "\n",
    "In PySpark, it is important to note that the Python data is stored within these JVM objects and these objects allow  any job to perform calculations very quickly.\n",
    "\n",
    "RDDs are:\n",
    "- calculated against\n",
    "- cached\n",
    "- stored in-memory\n",
    "\n",
    "At the same time, RDDs expose some coarse-gained transformations such as:\n",
    "- ```map(...)```\n",
    "- ```reduce(...)```\n",
    "- ```filter(...)```\n",
    "\n",
    "RDDs have two sets of parallel operations:\n",
    "- **transformations**(which return pointers to new RDDs) and\n",
    "- **actions**(which return values to the driver after running a computation)\n",
    "\n",
    "\n",
    "RDD transformation operations are lazy in a sense that they do not compute their results immediately. The transformations are only computed when an action is executed and the results need to be returned to the driver. This delayed execution results in more fine-tuned queries: Queries that are optimized for performance. \n",
    "\n",
    "- Spark automatically keeps track of how we create RDDs and automatically rebuils them if a machine fails or is running slow.\n",
    "\n",
    "## DataFrames\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "DataFrames, like RDDs, are immutable collections of data distributed among teh nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into named columns.\n",
    "\n",
    "\n",
    "DataFrames were designed to make large data sets processing even easier. They allow developers to formalize the structure of the data, allowing higher-level abstraction; in that sense DataFrames resemble tables from the relational database world. DataFrames provide a domain specific language API to manipulate the distributed data and make Spark accessible to a wider audience, beyond specialized data engineers.\n",
    "\n",
    "One of the major benefits of DataFrames is that the Spark Engine initially builds a logical execution plan and executes generated code based on a physical plan determined by a cost optimizer. Unlide RDDs that can be significantly slower on Python compared with Java or Scala.\n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "The goal of Spark Datasets is to provide an API that allows users to easily express transformations on domain objects, while also providing the performance and benefits of the robust Spark SQL execution engine. \n",
    "\n",
    "\n",
    "\n",
    "# Catalyst Optimizer\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "Spark SQL is one of the most technically involved components of Apache Spark as it powers both SQL queries and the DataFrame API. At the core of Spark SQL is the Catalyst Optimizer. The optimizer is based on functional programming constructs and was designed with two purposes in mind: \n",
    "- To ease the addition of new optimization techniques and features to Spark SQL and \n",
    "- to allow external developers to extend the optimizer (for example, adding data source specific rules, support for new data types, and so on):\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 2.0 architecture\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Unifying Datasets and DataFrames\n",
    "\n",
    "The history of the Spark APIs is denoted in the following diagram noting the progression from RDD to DataFrame to Dataset:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_06.jpg)\n",
    "\n",
    "*Source: From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD? http://bit.ly/29JPJSA*\n",
    "\n",
    " As you can see from the following diagram, DataFrame and Dataset both belong to the new Dataset API introduced as part of Apache Spark 2.0:\n",
    " ![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_07.jpg)\n",
    " \n",
    " *Source: A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets http://bit.ly/2accSNA*\n",
    " \n",
    "\n",
    " ## Structured Streaming\n",
    "  [back to top](#Table-of-Contents)\n",
    "  \n",
    "  \n",
    "As quoted by Reynold Xin during Spark Summit East 2016:\n",
    ">\"The simplest way to perform streaming analytics is not having to reason about streaming.\"\n",
    "\n",
    "This is the underlying foundation for building Structured Streaming. While streaming is powerful, one of the key issues is that streaming can be difficult to build and maintain. While companies such as Uber, Netflix, and Pinterest have Spark Streaming applications running in production, they also have dedicated teams to ensure the systems are highly available.\n",
    "\n",
    "### Spark Streaming: What Is It and Who’s Using It?\n",
    "\n",
    "![](https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2015/11/spark-streaming-datanami-300x169.png)\n",
    "*Spark Streaming ecosystem: Spark Streaming can consume static and streaming data from various sources, process data using Spark SQL and DataFrames, apply machine learning techniques from MLlib, and finally push out results to external data storage systems.*\n",
    "\n",
    "\n",
    "Streaming data is likely collected and used in batch jobs when generating daily reports and updating models. This means that a modern stream processing pipeline needs to be built, taking into account not just the real-time aspect, but also the associated pre-processing and post-processing aspects (e.g. model building).\n",
    "\n",
    "Before Spark Streaming, building complex pipelines that encompass streaming, batch, or even machine learning capabilities with open source software meant dealing with multiple frameworks, each built for a niche purpose, such as Storm for real-time actions, Hadoop MapReduce for batch processing, etc.\n",
    "\n",
    "Besides the pain of developing with disparate programming models, there was a huge cost of managing multiple frameworks in production. Spark and Spark Streaming, with its unified programming model and processing engine, makes all of this very simple.\n",
    " \n",
    "### Why Spark Streaming is Being Adopted Rapidly\n",
    "[back to top](#Table-of-Contents)\n",
    "  \n",
    "Spark Streaming was added to Apache Spark in 2013, an extension of the core Spark API that allows data engineers and data scientists to process real-time data from various sources like Kafka, Flume, and Amazon Kinesis. Its key abstraction is a Discretized Stream or, in short, a DStream, which represents a stream of data divided into small batches. DStreams are built on RDDs, Spark’s core data abstraction. This allows Spark Streaming to seamlessly integrate with any other Spark components like MLlib and Spark SQL.\n",
    "\n",
    "- This unification of disparate data processing capabilities is the key reason behind Spark Streaming’s rapid adoption. \n",
    " - It makes it very easy for developers to use a single framework to satisfy all the processing needs. \n",
    " - They can use MLlib (Spark’s machine learning library) to train models offline and directly spark_87use them online for scoring live data in Spark Streaming. In fact, some models perform continuous, online learning, and scoring. \n",
    " - Furthermore, data from streaming sources can be combined with a very large range of static data sources available through Spark SQL. For example, static data from Amazon Redshift can be loaded in memory in Spark and used to enrich the streaming data before pushing to downstream systems.\n",
    "\n",
    "- Last but not least, all the data collected can be later post-processed for report generation or queried interactively for ad-hoc analysis using Spark. \n",
    " - The code and business logic can be shared and reused between streaming, batch, and interactive processing pipelines. \n",
    " - In short, developers and system administrators can spend less time learning, implementing, and maintaining different frameworks, and focus on developing smarter applications.\n",
    " \n",
    " \n",
    "### Streaming Use Cases \n",
    "\n",
    "There are four broad ways Spark Streaming is being used today.\n",
    "\n",
    "\n",
    "- Streaming ETL – Data is continuously cleaned and aggregated before being pushed into data stores.\n",
    "- Triggers – Anomalous behavior is detected in real-time and further downstream actions are triggered accordingly. E.g. unusual behavior of sensor devices generating actions.\n",
    "- Data enrichment – Live data is enriched with more information by joining it with a static dataset allowing for a more complete real-time analysis.\n",
    "- Complex sessions and continuous learning – Events related to a live session \n",
    "\n",
    "Source: https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/\n",
    "\n",
    "### Continuous applications\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "Spark 2.0 has the ability to aggregate data into a stream and then serving it using traditional JDBC/ODBC, to change queries at run time, and/or to build and apply ML models in for many scenario in a variety of latency use cases:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/learning-pyspark/9781786463708/graphics/B05793_01_10.jpg)\n",
    "\n",
    "*Source: Apache Spark Key Terms, Explained https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
