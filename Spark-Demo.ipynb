{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [SparkSession](#SparkSession)\n",
    "2. [RDDs](#RDDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "\n",
    "In the past, you would potentially work with SparkConf, SparkContext, SQLContext, and HiveContext to execute your various Spark queries for configuration, Spark context, SQL context, and Hive context respectively. The SparkSession is a combination of these contexts including StreamingContext.\n",
    "\n",
    "The SparkSession is now the entry point for reading data, working with metadata, configuring the session, and managing the cluster resources.\n",
    "\n",
    "The SQLContext, HiveContext and StreamingContext still exist under the hood in Spark 2.0 for continuity purpose with the Spark legacy code.\n",
    "\n",
    "The Spark session has to be created when using spark-submit command. An example on how to do that is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    " \n",
    "spark = SparkSession.builder.appName(\"example-spark\").config(\"spark.sql.crossJoin.enabled\",\"true\").getOrCreate()\n",
    "#sc = SparkContext()\n",
    " \n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs\n",
    "\n",
    "RDDs operate in parallel. This is the strongest advantage of working in Spark: Each transformation is executed in parallel for enormous increase in speed.\n",
    "\n",
    "**The transformations to the dataset are lazy**. This means that any transformation is only executed when an action on a dataset is called. This helps Spark to optimize the execution.\n",
    "\n",
    "## Creating RDDs\n",
    "\n",
    "There are two ways to create an RDD in PySpark. You can either\n",
    "- ```.parallelize(...)``` a colection(list or an array of some elements):\n",
    "- or you can reference a file(or files located either locally or somewhere externally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"/Users/abanihiadmin/Documents/elnino.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Observation, Year, Month, Day, Date, Latitude, Longitude, Zonal Winds, Meridional Winds, Humidity, Air Temp, Sea Surface Temp',\n",
       " u'1,80,3,7,800307,-0.02,-109.46,-6.8,0.7,.,26.14,26.24',\n",
       " u'2,80,3,8,800308,-0.02,-109.46,-4.9,1.1,.,25.66,25.97',\n",
       " u'3,80,3,9,800309,-0.02,-109.46,-4.5,2.2,.,25.69,25.28',\n",
       " u'4,80,3,10,800310,-0.02,-109.46,-3.8,1.9,.,25.57,24.31']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameter in ```sc.textFile(..., n)``` specifies the number of partitions the dataset is divided into.\n",
    "\n",
    "**TIP: A rule of thumb would be to break your dataset into two-four partitions for each in your cluster.**\n",
    "\n",
    "Note: **When reading from a text file, each row from the file forms an element of an RDD.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "[back to top](#Table-of-Contents)\n",
    "\n",
    "## ```.map(...)```\n",
    "\n",
    "This method is applied to each element of the RDD: in the case for ```raw_data``` dataset we can think of this as a transformation of each row.\n",
    "\n",
    "\n",
    "By using the map transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular.\n",
    "\n",
    "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 0.04 seconds\n",
      "[[u'Observation',\n",
      "  u' Year',\n",
      "  u' Month',\n",
      "  u' Day',\n",
      "  u' Date',\n",
      "  u' Latitude',\n",
      "  u' Longitude',\n",
      "  u' Zonal Winds',\n",
      "  u' Meridional Winds',\n",
      "  u' Humidity',\n",
      "  u' Air Temp',\n",
      "  u' Sea Surface Temp'],\n",
      " [u'1',\n",
      "  u'80',\n",
      "  u'3',\n",
      "  u'7',\n",
      "  u'800307',\n",
      "  u'-0.02',\n",
      "  u'-109.46',\n",
      "  u'-6.8',\n",
      "  u'0.7',\n",
      "  u'.',\n",
      "  u'26.14',\n",
      "  u'26.24']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(2)\n",
    "tt = time() - t0\n",
    "print \"Parse completed in {} seconds\".format(round(tt,3))\n",
    "pprint(head_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all action happens once we call the first Spark action (i.e. take in this case). What if we take a lot of elements instead of just the first few?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 0.392 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(10000)\n",
    "tt = time() - t0\n",
    "print \"Parse completed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
